# Kubernetes Cluster Troubleshooting Guide

## üîç The Problem

**Initial Error:** `"No route to host"` when trying to SSH into the VM.

**Root Cause:** The VM was powered off. After turning it on, KVM (via DHCP) assigned it a new IP address, which broke the SSH config and the existing Kubernetes cluster (which relies on static IPs).

---

## üõ†Ô∏è The Resolution Cheat Sheet

### Step 1: Restore Connectivity

We found the VM was off, turned it on, and identified the new IP. 

**Check VM status:**
```bash
sudo virsh list --all
```

**Start the VMs:**
```bash
sudo virsh start k8s-master
sudo virsh start k8s-worker1
sudo virsh start k8s-worker2
```

**Find the actual IP address:**
```bash
sudo virsh domifaddr k8s-master
```

---

### Step 2: Lock the IPs (DHCP Reservation)

To prevent this issue from happening again, we configured KVM to permanently assign specific IPs to your VMs' MAC addresses. 

**Run on Host Machine (joyhomelab):**

```bash
# Lock Master
sudo virsh net-update default add ip-dhcp-host \
  "<host mac='52:54:00:15:03:d2' name='k8s-master' ip='192.168.122.90'/>" \
  --live --config

# Lock Worker 1
sudo virsh net-update default add ip-dhcp-host \
  "<host mac='52:54:00:db:e9:4f' name='k8s-worker1' ip='192.168.122.239'/>" \
  --live --config

# Lock Worker 2
sudo virsh net-update default add ip-dhcp-host \
  "<host mac='52:54:00:52:f6:71' name='k8s-worker2' ip='192.168.122.125'/>" \
  --live --config
```

> **Note:** We also updated `/etc/hosts` on the host machine to match these new IPs.

---

### Step 3: Fix the Broken Cluster (Master Node)

Because the IP changed, the old Kubernetes installation was broken. We had to reset and re-initialize it.

#### 3.1 SSH into Master

```bash
ssh joy@k8s-master
```

#### 3.2 Prepare the Node (Fix Swap & Containerd)

```bash
sudo swapoff -a

# Fix Containerd Config (The Runtime)
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo systemctl restart containerd
```

#### 3.3 Reset & Re-initialize Kubernetes

```bash
sudo kubeadm reset -f
sudo kubeadm init --pod-network-cidr=192.168.0.0/16
```

#### 3.4 Authorize User

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### 3.5 Install Network Plugin (Calico)

```bash
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml
```

---

### Step 4: Fix and Join the Workers

The workers were also broken. We had to reset them and join them to the new Master. 

#### 4.1 SSH into each Worker

```bash
ssh joy@k8s-worker1  # Repeat for k8s-worker2
```

#### 4.2 Run the Fix Sequence

```bash
sudo swapoff -a

# Fix Containerd
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo systemctl restart containerd

# Reset Old Config
sudo kubeadm reset -f

# Join the Cluster (Use the specific command generated by your Master)
sudo kubeadm join 192.168.122.90:6443 \
  --token <your-token> \
  --discovery-token-ca-cert-hash <your-hash>
```

> **Important:** Replace `<your-token>` and `<your-hash>` with the actual values from the `kubeadm init` output on the master node.

---

### Step 5: Verification ‚úÖ

**Check nodes status:**
```bash
kubectl get nodes
```
*All nodes should show `Ready` status.*

**Test with a sample deployment:**
```bash
kubectl create deployment nginx-test --image=nginx
kubectl get pods
```
*Pod should be in `Running` state.*

---

## üìù Summary

| Issue | Solution |
|-------|----------|
| VMs powered off | Started VMs with `virsh start` |
| Dynamic IPs changing | Set DHCP reservations with `virsh net-update` |
| Broken cluster after IP change | Reset and re-initialized with `kubeadm` |
| Workers unable to join | Reset workers and joined with new token |

---

## üîó IP Assignment Table

| Node | MAC Address | Static IP |
|------|-------------|-----------|
| k8s-master | `52:54:00:15:03:d2` | `192.168.122.90` |
| k8s-worker1 | `52:54:00:db:e9:4f` | `192.168.122.239` |
| k8s-worker2 | `52:54:00:52:f6:71` | `192.168.122.125` |
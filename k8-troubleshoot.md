The Problem
Initial Error: "No route to host" when trying to SSH.

Root Cause: The VM was powered off. After turning it on, KVM (via DHCP) assigned it a new IP address, which broke your SSH config and the existing Kubernetes cluster (which relies on static IPs).
The Resolution Cheat Sheet
Step 1: Restore Connectivity
We found the VM was off, turned it on, and identified the new IP.

Check VM status: sudo virsh list --all
Start the VMs:
sudo virsh start k8s-master
sudo virsh start k8s-worker1
sudo virsh start k8s-worker2

Find the actual IP address:sudo virsh domifaddr k8s-master
Step 2: Lock the IPs (DHCP Reservation)
To prevent this issue from happening again, we told KVM to permanently assign specific IPs to your VMs' MAC addresses.

Run on Host Machine (joyhomelab):
# Lock Master
sudo virsh net-update default add ip-dhcp-host "<host mac='52:54:00:15:03:d2' name='k8s-master' ip='192.168.122.90'/>" --live --config

# Lock Worker 1
sudo virsh net-update default add ip-dhcp-host "<host mac='52:54:00:db:e9:4f' name='k8s-worker1' ip='192.168.122.239'/>" --live --config

# Lock Worker 2
sudo virsh net-update default add ip-dhcp-host "<host mac='52:54:00:52:f6:71' name='k8s-worker2' ip='192.168.122.125'/>" --live --config
Action: We also updated /etc/hosts on your host machine to match these new IPs.

Step 3: Fix the Broken Cluster (Master Node)
Because the IP changed, the old Kubernetes installation was broken. We had to reset and re-initialize it.

SSH into Master: ssh joy@k8s-master

Prepare the Node (Fix Swap & Containerd):

Bash

sudo swapoff -a

# Fix Containerd Config (The Runtime)
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo systemctl restart containerd
Reset & Re-init Kubernetes:

Bash

sudo kubeadm reset -f
sudo kubeadm init --pod-network-cidr=192.168.0.0/16
Authorize User:

Bash

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Install Network Plugin (Calico):

Bash

kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml
Step 4: Fix and Join the Workers
The workers were also broken. We had to reset them and join them to the new Master.

SSH into each Worker: ssh joy@k8s-worker1 (and then worker2)

Run the Fix Sequence:

Bash

sudo swapoff -a

# Fix Containerd
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo systemctl restart containerd

# Reset Old Config
sudo kubeadm reset -f

# Join the Cluster (Use the specific command generated by your Master)
sudo kubeadm join 192.168.122.90:6443 --token <your-token> --discovery-token-ca-cert-hash <your-hash>
Step 5: Verification
Check Nodes: kubectl get nodes (On Master) -> All should be Ready.

Test App: kubectl create deployment nginx-test --image=nginx -> Should be Running.
